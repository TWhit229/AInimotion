# LayeredInterpolator Training Configuration
# ============================================
# Optimized for RTX 5070 Ti (16GB VRAM)

# Model Architecture
base_channels: 64        # Maximum capacity for quality
kernel_size: 7           # AdaCoF kernel size (KÃ—K)
grid_size: 8             # Background affine grid size
use_refinement: true     # Use refinement U-Net in compositor

# Discriminator
disc_channels: 64        # Base discriminator channels

# Training
epochs: 30               # Reduced from 100 (most learning in early epochs)
batch_size: 6            # Reduced for larger model (more VRAM needed)
num_workers: 12          # High parallelism for fast data loading
crop_size: [256, 256]    # Random crop size for training
prefetch_factor: 4       # Prefetch batches per worker
persistent_workers: true # Keep workers alive between epochs
max_samples: 100000      # Limit dataset size for faster training

# Learning rates
lr_g: 0.0001             # Generator learning rate
lr_d: 0.00005            # Lower D learning rate to prevent domination
weight_decay: 0.01       # AdamW weight decay

# Gradient clipping
grad_clip: 1.0           # Clip gradients to prevent explosion (was inf)

# Loss weights
l1_weight: 1.0           # L1 reconstruction loss
perceptual_weight: 0.1   # VGG perceptual loss
edge_weight: 0.5         # Edge-weighted L1 loss
gan_weight: 0.01         # Adversarial loss weight

# GAN configuration
gan_type: lsgan          # 'vanilla', 'lsgan', or 'hinge'

# Mixed precision
use_amp: true            # Use automatic mixed precision (FP16)
cudnn_benchmark: true    # Enable cuDNN auto-tuner for faster training

# Logging and checkpointing
log_dir: runs
checkpoint_dir: checkpoints
save_every: 1            # Save checkpoint every epoch
save_every_batches: 5000 # Save every N batches (increased for fewer saves)

# Data
data_dir: D:\Triplets    # Path to triplet dataset

