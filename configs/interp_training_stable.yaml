# LayeredInterpolator Training Configuration
# ============================================
# STABLE VERSION - Lower LR to reduce gradient explosions
# Optimized for RTX 5070 Ti (16GB VRAM)

# Model Architecture
base_channels: 64        # Maximum capacity for best quality
kernel_size: 7           # AdaCoF kernel size (KÃ—K)
grid_size: 8             # Background affine grid size
use_refinement: true     # Use refinement U-Net in compositor

# Discriminator
disc_channels: 64        # Base discriminator channels

# Training
epochs: 50               # Extended for better quality
batch_size: 4            # Smaller batch for 64ch model
num_workers: 12          # High parallelism for fast data loading
crop_size: [256, 256]    # Random crop size for training
prefetch_factor: 4       # Prefetch batches per worker
persistent_workers: true # Keep workers alive between epochs
max_samples: 100000      # Limit dataset size for faster training

# Learning rates
lr_g: 0.00005            # Reduced from 0.0001 for stability
lr_d: 0.00001            # Very slow D to prevent inf gradients
weight_decay: 0.01       # AdamW weight decay

# Gradient clipping
grad_clip: 1.0           # Clip gradients to prevent explosion (was inf)

# Loss weights
l1_weight: 1.0           # L1 reconstruction loss
perceptual_weight: 0.1   # VGG perceptual loss
edge_weight: 1.0         # Edge-weighted L1 loss (doubled for sharper edges)
edge_multiplier: 20.0    # Edge pixel weight (doubled from 10)
gan_weight: 0.01         # Adversarial loss weight

# GAN configuration
gan_type: lsgan          # 'vanilla', 'lsgan', or 'hinge'
label_smoothing: 0.1     # Makes D's task harder (prevents overconfidence)
d_update_ratio: 2        # Update D every N batches (G trains more often)

# Mixed precision
use_amp: true            # Use automatic mixed precision (FP16)
cudnn_benchmark: true    # Enable cuDNN auto-tuner for faster training

# Logging and checkpointing
log_dir: runs
checkpoint_dir: checkpoints
save_every: 1            # Save checkpoint every epoch
save_every_batches: 5000 # Save every N batches (increased for fewer saves)

# Data
data_dir: D:\Triplets    # Path to triplet dataset

