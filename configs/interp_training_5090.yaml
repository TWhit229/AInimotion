# LayeredInterpolator Training Configuration
# ============================================
# Optimized for RTX 5090 (32GB VRAM)

# Model Architecture
base_channels: 96        # Maximum capacity for 32GB VRAM
kernel_size: 9           # AdaCoF kernel size (K×K) — sweep winner: K=9 > K=7
grid_size: 8             # Background affine grid size
use_refinement: true     # Use refinement U-Net in compositor

# Discriminator
disc_channels: 64        # Base discriminator channels

# Training
epochs: 50               # Extended for better quality
batch_size: 6            # Sweep winner (384 crops need smaller batch)
num_workers: 12          # High parallelism for fast data loading
crop_size: [384, 384]    # Sweep winner: +0.7 dB PSNR over 256 crops
prefetch_factor: 4       # Prefetch batches per worker
persistent_workers: true # Keep workers alive between epochs
max_samples: 100000      # Limit dataset size for faster training

# Learning rates
lr_g: 0.0003             # Sweep winner: lr=3e-4 slightly beat 2e-4
lr_d: 0.00002            # Much slower D to prevent domination
weight_decay: 0.01       # AdamW weight decay

# Gradient clipping
grad_clip: 1.0           # Clip gradients to prevent explosion (was inf)

# Loss weights
l1_weight: 1.5           # Sweep winner: balanced with perceptual (was 2.0)
perceptual_weight: 0.1   # Sweep winner: higher perceptual actually helps PSNR (was 0.05)
edge_weight: 1.0         # Edge-weighted L1 loss (doubled for sharper edges)
edge_multiplier: 20.0    # Edge pixel weight (doubled from 10)
gan_weight: 0.005        # Lower GAN weight for Phase 2 fine-tuning

# GAN configuration — R3GAN-inspired (arXiv 2501.05441)
gan_start_epoch: 35      # No GAN for epochs 0-34, enable at epoch 35
gan_type: relativistic   # RpGAN loss (couples real/fake for mode coverage)
r1_gamma: 10.0           # R1 gradient penalty on real data (convergence guarantee)
r2_gamma: 10.0           # R2 gradient penalty on fake data (convergence guarantee)
lr_g_phase2: 0.00005     # Lower lr_g when GAN activates for stable fine-tuning
# Old ad-hoc workarounds (no longer needed with R1+R2 penalties):
# label_smoothing: 0.1   # Replaced by R1/R2 penalties
# d_update_ratio: 2      # Replaced by R1/R2 penalties

# Phase 2 VRAM management (gradient accumulation)
phase2_batch_size: 3     # Physical batch during Phase 2 (discriminator needs VRAM)
gradient_accumulation_steps: 2  # Accumulate 2 mini-batches → effective batch = 6

# Mixed precision — BFloat16 recommended by R3GAN paper (FP16 "cripples" their loss)
use_amp: true
amp_dtype: bfloat16      # BFloat16 = wider range than FP16, no loss scaling needed
cudnn_benchmark: true    # Enable cuDNN auto-tuner for faster training

# Logging and checkpointing
log_dir: runs
checkpoint_dir: checkpoints
save_every: 1            # Save checkpoint every epoch
save_every_batches: 5000 # Save every N batches (increased for fewer saves)

# Data
data_dir: D:\Triplets    # Path to triplet dataset

