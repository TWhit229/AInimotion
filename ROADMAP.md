# AInimotion Development Roadmap

A practical guide from empty repo to working anime video enhancer.

---

## Phase 0: Project Setup âœ…
*Get the skeleton in place*

- [x] Create the package structure
- [x] Set up `pyproject.toml` â€” defines dependencies, entry points, metadata
- [x] Create a dev environment
- [x] Verify FFmpeg is available â€” required for decode/encode

---

## Phase 0.5: Training Data Pipeline âœ… COMPLETE
*Build the triplet extraction system for model training*

| Step | Task | Status |
|------|------|--------|
| 0.5.1 | **Triplet extraction script** â€” extract F1, F2, F3 frame triplets | âœ… |
| 0.5.2 | **SSIM-based filtering** â€” filter out static/duplicate frames | âœ… |
| 0.5.3 | **Parallel processing** â€” 8-worker ThreadPoolExecutor | âœ… |
| 0.5.4 | **720p output** â€” extract frames at 720p for efficiency | âœ… |
| 0.5.5 | **Chunked processing** â€” 5-minute chunks to manage temp storage | âœ… |
| 0.5.6 | **Skip intro** â€” skip first 120s to avoid logos/credits | âœ… |
| 0.5.7 | **Hardsub support** â€” crop top/bottom to remove burned-in subs | âœ… |
| 0.5.8 | **Helper scripts** â€” PowerShell scripts for batch processing | âœ… |
| 0.5.9 | **Progress bars** â€” tqdm progress for video/chunk/filtering | âœ… |
| 0.5.10 | **JPEG output** â€” 10x smaller files with quality 95 default | âœ… |

**Milestone:** âœ… **65,906 triplets extracted** from diverse anime sources!

### Dataset Statistics
- **Total triplets:** 65,906
- **Format:** PNG (existing) / JPEG (new default)
- **Resolution:** 720p
- **Sources:** Movies, TV series, various studios

---

## Phase 1: Build the Pipeline Skeleton âœ… COMPLETE
*Wire up the stages with placeholder logic*

**Goal:** Input video â†’ goes through all stages â†’ output video (even if no processing happens yet)

| Step | Task | Status |
|------|------|--------|
| 1.1 | **FFmpeg decode** â€” extract frames to temp dir or memory | âœ… |
| 1.2 | **FFmpeg encode** â€” take frames and encode back to video + remux audio | âœ… |
| 1.3 | **CLI entry point** â€” `ainimotion enhance input.mkv --out output.mkv` does round-trip | âœ… |

**Milestone:** âœ… CLI runs with `ainimotion enhance` and `ainimotion info` commands.

---

## Phase 2: Add Gating (The "Anime-Safe" Core) âœ… COMPLETE
*Detect scene cuts and holds BEFORE adding any ML*

| Step | Task | Status |
|------|------|--------|
| 2.1 | **Frame comparison** â€” downscale + compute luma similarity (SSIM or MAD) | âœ… |
| 2.2 | **Classify intervals** â€” cut / hold / motion based on thresholds | âœ… |
| 2.3 | **Frame duplication** â€” for 24â†’48 FPS, duplicate frames for cuts/holds | âœ… |
| 2.4 | **Unit tests** â€” test gating logic with synthetic frame pairs | âœ… |

**Milestone:** âœ… Gating module detects CUT/HOLD/MOTION. Ready for interpolation integration.

---

## Phase 3: Plug In Interpolation (Model A)
*Add the ML model for generating in-between frames*

| Step | Task | Status |
|------|------|--------|
| 3.1 | **Choose baseline model** â€” RIFE, IFRNet, or similar (pretrained) | [ ] |
| 3.2 | **Integrate inference** â€” PyTorch or ONNX Runtime | [ ] |
| 3.3 | **Wire into pipeline** â€” when gating says "motion", call the model | [ ] |
| 3.4 | **Test on real clips** â€” verify interpolation quality on anime footage | [ ] |

**Milestone:** Real interpolation for motion, duplication for holds/cuts. First "enhanced" outputs.

---

## Phase 4: Plug In Upscaling (Model B)
*Add the upscaler/restorer*

| Step | Task | Status |
|------|------|--------|
| 4.1 | **Choose baseline model** â€” Real-ESRGAN, Compact, or anime-tuned variant | [ ] |
| 4.2 | **Add tiling** â€” handle large frames without OOM | [ ] |
| 4.3 | **Wire after interpolation** â€” upscale every output frame | [ ] |
| 4.4 | **Test on real clips** â€” verify upscale quality, check for artifacts | [ ] |

**Milestone:** Full pipeline working: Interpolate â†’ Upscale. End-to-end 1080p@24fps â†’ 1440p@48fps.

---

## Phase 5: Model Training âš¡ CURRENT
*Train custom models on extracted triplets*

| Step | Task | Status |
|------|------|--------|
| 5.1 | **Dataset loader** â€” load triplets for training | âœ… |
| 5.2 | **Training loop** â€” implement training with loss functions | âœ… |
| 5.3 | **Mixed precision** â€” FP16/FP32 via GradScaler | âœ… |
| 5.4 | **Checkpointing** â€” per-epoch + per-batch, auto-resume | âœ… |
| 5.5 | **W&B + TensorBoard** â€” real-time loss/PSNR monitoring | âœ… |
| 5.6 | **Two-phase training** â€” Phase 1 reconstruction, Phase 2 GAN | âœ… |
| 5.7 | **GAN stabilization** â€” LSGAN, label smoothing, adaptive lr_d, D warmup | âœ… |
| 5.8 | **OOM recovery** â€” auto-checkpoint + retry with backoff | âœ… |
| 5.9 | **Hyperparameter sweep** â€” 13 experiments, automated comparison | âœ… |
| 5.10 | **Gradient accumulation** â€” Phase 2 VRAM management | âœ… |
| 5.11 | **Full training run** â€” 50 epochs with sweep-winner config | ðŸ”„ In Progress |
| 5.12 | **Validation** â€” test on held-out anime clips | [ ] |

### Sweep Results (13 Experiments)

| Rank | Config | PSNR (5 epochs) |
|------|--------|------------------|
| ðŸ¥‡ 1 | **heavy_motion_max** (K=9, crop 384, perc=0.1) | **21.80 dB** |
| ðŸ¥ˆ 2 | motion_focused (crop 384, perc=0.1) | 21.73 dB |
| ðŸ¥‰ 3 | crop_384 (384 crops only) | 21.69 dB |
| 11 | lr_baseline (current defaults) | 21.06 dB |

**Key finding:** 384Ã—384 crops (+0.74 dB), K=9 kernels, and higher perceptual weight were the biggest wins.

### Training Configuration (Sweep Winner)
- **Model:** LayeredInterpolator (96 base channels, K=9 AdaCoF)
- **Dataset:** 65,906 triplets at 720p
- **GPU:** RTX 5090 (32 GB VRAM)
- **Phase 1:** Batch 6, crop 384Ã—384, lr=3e-4, 35 epochs
- **Phase 2:** Batch 3 + 2Ã— gradient accumulation (effective 6), lr=5e-5, 15 epochs
- **Losses:** L1=1.5, Perceptual=0.1, Edge=1.0 (20Ã— multiplier), GAN=0.005 (Phase 2)

**Milestone:** Custom anime-trained interpolation model ready for inference.

---

## Phase 6: Harden Quality + Performance
*Make it actually good for anime*

| Step | Task | Status |
|------|------|--------|
| 6.1 | **Tune gating thresholds** â€” balance between false cuts and false motion | [ ] |
| 6.2 | **Bad mid-frame detection** â€” if interpolated frame looks wrong, fallback to duplicate | [ ] |
| 6.3 | **FP16 inference** â€” reduce VRAM usage and improve speed | [ ] |
| 6.4 | **Memory-aware batching** â€” auto tile sizing based on available VRAM | [ ] |
| 6.5 | **NVENC encoding** â€” optional GPU encoding for speed presets | [ ] |
| 6.6 | **Benchmark suite** â€” measure FPS, VRAM, quality metrics | [ ] |

**Milestone:** Pipeline is stable, fast, and produces high-quality anime-safe output.

---

## Phase 7: Polish + UX
*Make it usable*

| Step | Task | Status |
|------|------|--------|
| 7.1 | **Better CLI UX** â€” progress bars, ETA, logging levels | [ ] |
| 7.2 | **Presets implementation** â€” `anime-clean`, `anime-strong`, `fast` | [ ] |
| 7.3 | **Error handling** â€” corrupt files, unsupported codecs, VRAM failures | [ ] |
| 7.4 | **Clip mode** â€” `--clip 00:10:00-00:10:20` for preview renders | [ ] |
| 7.5 | **Documentation** â€” usage examples, troubleshooting, FAQ | [ ] |

**Milestone:** AInimotion is ready for public release as a CLI tool.

---

## Phase 8: GUI + Packaging
*Desktop application*

| Step | Task | Status |
|------|------|--------|
| 8.1 | Drag/drop desktop UI | [ ] |
| 8.2 | Short preview renders in-app | [ ] |
| 8.3 | Presets panel + advanced settings | [ ] |
| 8.4 | Bundle FFmpeg + runtime + weights | [ ] |
| 8.5 | Installer for Windows/macOS/Linux | [ ] |

---

## Quick Reference: Tech Stack

| Component | Technology |
|-----------|------------|
| Language | Python 3.10+ |
| Video I/O | FFmpeg (subprocess) |
| Training | PyTorch 2.9+ with CUDA 12.8+ |
| Inference (prod) | ONNX Runtime + TensorRT |
| Interpolation | LayeredInterpolator (custom: FPN + AdaCoF + Affine Grid) |
| Upscaling | Real-ESRGAN / Compact (or custom) |
| Encoding | FFmpeg (libx264/libx265) or NVENC |
| Monitoring | Weights & Biases + TensorBoard |
| GPU | NVIDIA RTX 5090 (32 GB) |

---

## Getting Started

**Current focus: Phase 5 (Model Training)**

Full training is in progress with sweep-optimized config:

```powershell
cd "C:\Projects\AInimotion"
python -m ainimotion.training.train --config configs/interp_training_5090.yaml --data "D:\Triplets" --wandb --auto-resume
```
