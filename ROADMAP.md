# AInimotion Development Roadmap

A practical guide from empty repo to working anime video enhancer.

---

## Phase 0: Project Setup âœ…
*Get the skeleton in place*

- [x] Create the package structure
- [x] Set up `pyproject.toml` â€” defines dependencies, entry points, metadata
- [x] Create a dev environment
- [x] Verify FFmpeg is available â€” required for decode/encode

---

## Phase 0.5: Training Data Pipeline âœ… COMPLETE
*Build the triplet extraction system for model training*

| Step | Task | Status |
|------|------|--------|
| 0.5.1 | **Triplet extraction script** â€” extract F1, F2, F3 frame triplets | âœ… |
| 0.5.2 | **SSIM-based filtering** â€” filter out static/duplicate frames | âœ… |
| 0.5.3 | **Parallel processing** â€” 8-worker ThreadPoolExecutor | âœ… |
| 0.5.4 | **720p output** â€” extract frames at 720p for efficiency | âœ… |
| 0.5.5 | **Chunked processing** â€” 5-minute chunks to manage temp storage | âœ… |
| 0.5.6 | **Skip intro** â€” skip first 120s to avoid logos/credits | âœ… |
| 0.5.7 | **Hardsub support** â€” crop top/bottom to remove burned-in subs | âœ… |
| 0.5.8 | **Helper scripts** â€” PowerShell scripts for batch processing | âœ… |
| 0.5.9 | **Progress bars** â€” tqdm progress for video/chunk/filtering | âœ… |
| 0.5.10 | **JPEG output** â€” 10x smaller files with quality 95 default | âœ… |

**Milestone:** âœ… **65,906 triplets extracted** from diverse anime sources!

### Dataset Statistics
- **Total triplets:** 65,906
- **Format:** PNG (existing) / JPEG (new default)
- **Resolution:** 720p
- **Sources:** Movies, TV series, various studios

---

## Phase 1: Build the Pipeline Skeleton âœ… COMPLETE
*Wire up the stages with placeholder logic*

**Goal:** Input video â†’ goes through all stages â†’ output video (even if no processing happens yet)

| Step | Task | Status |
|------|------|--------|
| 1.1 | **FFmpeg decode** â€” extract frames to temp dir or memory | âœ… |
| 1.2 | **FFmpeg encode** â€” take frames and encode back to video + remux audio | âœ… |
| 1.3 | **CLI entry point** â€” `ainimotion enhance input.mkv --out output.mkv` does round-trip | âœ… |

**Milestone:** âœ… CLI runs with `ainimotion enhance` and `ainimotion info` commands.

---

## Phase 2: Add Gating (The "Anime-Safe" Core)
*Detect scene cuts and holds BEFORE adding any ML*

| Step | Task | Status |
|------|------|--------|
| 2.1 | **Frame comparison** â€” downscale + compute luma similarity (SSIM or MAD) | [ ] |
| 2.2 | **Classify intervals** â€” cut / hold / motion based on thresholds | [ ] |
| 2.3 | **Frame duplication** â€” for 24â†’48 FPS, duplicate frames for cuts/holds | [ ] |
| 2.4 | **Unit tests** â€” test gating logic with synthetic frame pairs | [ ] |

**Milestone:** Output is 48 FPS via intelligent duplication. No interpolation yet, but the gating logic is solid.

---

## Phase 3: Plug In Interpolation (Model A)
*Add the ML model for generating in-between frames*

| Step | Task | Status |
|------|------|--------|
| 3.1 | **Choose baseline model** â€” RIFE, IFRNet, or similar (pretrained) | [ ] |
| 3.2 | **Integrate inference** â€” PyTorch or ONNX Runtime | [ ] |
| 3.3 | **Wire into pipeline** â€” when gating says "motion", call the model | [ ] |
| 3.4 | **Test on real clips** â€” verify interpolation quality on anime footage | [ ] |

**Milestone:** Real interpolation for motion, duplication for holds/cuts. First "enhanced" outputs.

---

## Phase 4: Plug In Upscaling (Model B)
*Add the upscaler/restorer*

| Step | Task | Status |
|------|------|--------|
| 4.1 | **Choose baseline model** â€” Real-ESRGAN, Compact, or anime-tuned variant | [ ] |
| 4.2 | **Add tiling** â€” handle large frames without OOM | [ ] |
| 4.3 | **Wire after interpolation** â€” upscale every output frame | [ ] |
| 4.4 | **Test on real clips** â€” verify upscale quality, check for artifacts | [ ] |

**Milestone:** Full pipeline working: Interpolate â†’ Upscale. End-to-end 1080p@24fps â†’ 1440p@48fps.

---

## Phase 5: Model Training âš¡ CURRENT
*Train custom models on extracted triplets*

| Step | Task | Status |
|------|------|--------|
| 5.1 | **Dataset loader** â€” load triplets for training | âœ… |
| 5.2 | **Training loop** â€” implement training with loss functions | âœ… |
| 5.3 | **Progress bars** â€” tqdm for epochs and batches | âœ… |
| 5.4 | **Mixed precision** â€” FP16 for faster training | âœ… |
| 5.5 | **Checkpointing** â€” save/resume training | âœ… |
| 5.6 | **TensorBoard** â€” real-time loss visualization | âœ… |
| 5.7 | **Train interpolator** â€” train on 65K triplets | ðŸ”„ In Progress |
| 5.8 | **Validation** â€” test on held-out anime clips | [ ] |

### Training Configuration
- **Model:** LayeredInterpolator (FPN + SceneGate + AdaCoF)
- **Dataset:** 65,906 triplets at 720p
- **GPU:** RTX 5070 Ti (16GB VRAM)
- **Batch size:** 16
- **Epochs:** 100
- **ETA:** ~12-24 hours

**Milestone:** Custom anime-trained interpolation model ready for inference.

---

## Phase 6: Harden Quality + Performance
*Make it actually good for anime*

| Step | Task | Status |
|------|------|--------|
| 6.1 | **Tune gating thresholds** â€” balance between false cuts and false motion | [ ] |
| 6.2 | **Bad mid-frame detection** â€” if interpolated frame looks wrong, fallback to duplicate | [ ] |
| 6.3 | **FP16 inference** â€” reduce VRAM usage and improve speed | [ ] |
| 6.4 | **Memory-aware batching** â€” auto tile sizing based on available VRAM | [ ] |
| 6.5 | **NVENC encoding** â€” optional GPU encoding for speed presets | [ ] |
| 6.6 | **Benchmark suite** â€” measure FPS, VRAM, quality metrics | [ ] |

**Milestone:** Pipeline is stable, fast, and produces high-quality anime-safe output.

---

## Phase 7: Polish + UX
*Make it usable*

| Step | Task | Status |
|------|------|--------|
| 7.1 | **Better CLI UX** â€” progress bars, ETA, logging levels | [ ] |
| 7.2 | **Presets implementation** â€” `anime-clean`, `anime-strong`, `fast` | [ ] |
| 7.3 | **Error handling** â€” corrupt files, unsupported codecs, VRAM failures | [ ] |
| 7.4 | **Clip mode** â€” `--clip 00:10:00-00:10:20` for preview renders | [ ] |
| 7.5 | **Documentation** â€” usage examples, troubleshooting, FAQ | [ ] |

**Milestone:** AInimotion is ready for public release as a CLI tool.

---

## Phase 8: GUI + Packaging
*Desktop application*

| Step | Task | Status |
|------|------|--------|
| 8.1 | Drag/drop desktop UI | [ ] |
| 8.2 | Short preview renders in-app | [ ] |
| 8.3 | Presets panel + advanced settings | [ ] |
| 8.4 | Bundle FFmpeg + runtime + weights | [ ] |
| 8.5 | Installer for Windows/macOS/Linux | [ ] |

---

## Quick Reference: Tech Stack

| Component | Technology |
|-----------|------------|
| Language | Python 3.10+ |
| Video I/O | FFmpeg (subprocess) |
| Inference (dev) | PyTorch + CUDA |
| Inference (prod) | ONNX Runtime + TensorRT |
| Interpolation | RIFE / IFRNet (or custom) |
| Upscaling | Real-ESRGAN / Compact (or custom) |
| Encoding | FFmpeg (libx264/libx265) or NVENC |
| Parallel Processing | concurrent.futures.ThreadPoolExecutor |

---

## Getting Started

**Current focus: Phase 0.5 (Training Data)**

The triplet extraction pipeline is complete. Run extraction with:

```powershell
cd "D:\Projects\AInimotion"
.\scripts\run_all_overnight.ps1
```

Or manually:

```bash
python scripts/extract_triplets.py \
  --input "/path/to/anime" \
  --output "/path/to/triplets" \
  --temp-dir "/path/to/temp" \
  --workers 8
```
